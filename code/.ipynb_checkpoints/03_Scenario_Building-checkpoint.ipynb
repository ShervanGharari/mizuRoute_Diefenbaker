{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc06c293",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Scenario building of the lake level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2236c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the packages\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "#import datetime #import date\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib\n",
    "from ipynb.fs.full.Untitled2 import save_netcdf\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 20}\n",
    "# matplotlib.rcs('font', **font)\n",
    "plt.rc('font', **font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9317baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating the target volumes for the lake Difenbaker\n",
    "# df = pd.read_csv('../data/HYDAT/05HF003_level.csv') # elevation record of lake diefenbaker\n",
    "\n",
    "# # set the date as the index\n",
    "# df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "# df = df.set_index('DATE')\n",
    "# df = df.loc['2013-01-01':'2013-12-31']\n",
    "# df.plot()\n",
    "\n",
    "\n",
    "# #\n",
    "# df_scenarios = df.copy()\n",
    "\n",
    "# # create the backward role of delta_time\n",
    "# lead_times = [0,1,2,3,4,5]\n",
    "# normalization_times = [1,2,3,4,5,6,7,8,9,10,]\n",
    "# ele_steps = [0,0.5,1,1.5,2]\n",
    "\n",
    "\n",
    "# #\n",
    "# date_of_interest = pd.to_datetime('2013-06-24')\n",
    "# delta = df.loc[date_of_interest,'05HF003'] - \\\n",
    "#         df.loc[date_of_interest-pd.Timedelta(days=1),'05HF003']\n",
    "\n",
    "# for lead_time in lead_times:\n",
    "#     for normalization_time in normalization_times:\n",
    "#         for ele_step in ele_steps:\n",
    "\n",
    "#             # craete the column name\n",
    "#             column_name = 'scenario_' + str(ele_step) +'_'+ str(lead_time)+ '_' + str(normalization_time)\n",
    "#             df_scenarios [column_name] = np.nan\n",
    "#             print(column_name)\n",
    "#             #print(df_scenarios)\n",
    "\n",
    "#             # shift back to the lead time from 24 of June\n",
    "#             df_scenarios.loc[:date_of_interest-pd.Timedelta(days=lead_time),column_name] = \\\n",
    "#             df_scenarios.loc[:date_of_interest,'05HF003'].shift(-1*lead_time)\n",
    "\n",
    "#             # create the model continuation\n",
    "#             df_scenarios.loc[date_of_interest-pd.Timedelta(days=lead_time):date_of_interest,column_name] = \\\n",
    "#             df_scenarios.loc[date_of_interest-pd.Timedelta(days=lead_time):date_of_interest,column_name]\n",
    "#             for n in np.arange(lead_time+1):\n",
    "#                 df_scenarios.loc[date_of_interest-pd.Timedelta(days=n),column_name] = \\\n",
    "#                 df_scenarios.loc[date_of_interest-pd.Timedelta(days=lead_time),column_name] + delta * (lead_time-n) \n",
    "\n",
    "#             # reduce elevation\n",
    "#             df_scenarios.loc[:date_of_interest,column_name] = df_scenarios.loc[:date_of_interest,column_name] - ele_step\n",
    "\n",
    "#             #\n",
    "#             df_scenarios.loc[date_of_interest:,column_name] = df_scenarios.loc[date_of_interest,column_name]\n",
    "#             df_scenarios['temp'] = np.nan\n",
    "#             df_scenarios.loc[:date_of_interest,'temp'] = 1\n",
    "#             df_scenarios.loc[date_of_interest+pd.Timedelta(days=normalization_time):,'temp'] = 0\n",
    "#             df_scenarios['temp'] = df_scenarios['temp'].interpolate()\n",
    "#             df_scenarios['temp'] = df_scenarios['temp']**3.0\n",
    "\n",
    "#             df_scenarios[column_name] = (df_scenarios['temp'])*df_scenarios[column_name]+(1-df_scenarios['temp'])*df_scenarios['05HF003']\n",
    "\n",
    "#             df_scenarios = df_scenarios.drop(columns='temp')\n",
    "\n",
    "# # craeting the varibales\n",
    "# df_scenarios_24 = df_scenarios.copy()\n",
    "# df_scenarios_24.columns = [col + '_24' if col.startswith('scenario') else col for col in df_scenarios_24.columns]\n",
    "\n",
    "# df_scenarios_25 = df_scenarios.copy()\n",
    "# df_scenarios_25 = df_scenarios_25.drop(columns='05HF003')\n",
    "# df_scenarios_25.columns = [col + '_25' if col.startswith('scenario') else col for col in df_scenarios_25.columns]\n",
    "# df_scenarios_25 = df_scenarios_25.shift(1)\n",
    "\n",
    "# df_scenarios_23 = df_scenarios.copy()\n",
    "# df_scenarios_23 = df_scenarios_23.drop(columns='05HF003')\n",
    "# df_scenarios_23.columns = [col + '_23' if col.startswith('scenario') else col for col in df_scenarios_23.columns]\n",
    "# df_scenarios_23 = df_scenarios_23.shift(-1)\n",
    "\n",
    "# df_scenarios_22 = df_scenarios.copy()\n",
    "# df_scenarios_22 = df_scenarios_22.drop(columns='05HF003')\n",
    "# df_scenarios_22.columns = [col + '_22' if col.startswith('scenario') else col for col in df_scenarios_22.columns]\n",
    "# df_scenarios_22 = df_scenarios_22.shift(-2)\n",
    "\n",
    "# df_scenarios_26 = df_scenarios.copy()\n",
    "# df_scenarios_26 = df_scenarios_26.drop(columns='05HF003')\n",
    "# df_scenarios_26.columns = [col + '_26' if col.startswith('scenario') else col for col in df_scenarios_26.columns]\n",
    "# df_scenarios_26 = df_scenarios_26.shift(+2)\n",
    "\n",
    "# df_scenarios = pd.concat([df_scenarios_22, df_scenarios_23, df_scenarios_24, df_scenarios_25, df_scenarios_26], axis=1)\n",
    "\n",
    "\n",
    "# # plotting\n",
    "# print(df_scenarios.columns)\n",
    "# df_scenarios['2013-06-15':'2013-07-10'].plot(figsize=(20,10))\n",
    "# plt.grid(which='both')\n",
    "# plt.gca().get_legend().remove()\n",
    "# plt.ylabel('Elevation of \\n lake Diefenbaker [m]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde403b9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_target_time (df, target_datetime, shifted_datetime, start_datetime, end_datetime):\n",
    "    \n",
    "    # shift before shifted_datetime\n",
    "    df1 = shift_target_time_one_side (df, target_datetime, shifted_datetime, start_datetime)\n",
    "    df2 = shift_target_time_one_side (df1, target_datetime, shifted_datetime, end_datetime)\n",
    "    \n",
    "    return df2\n",
    "\n",
    "\n",
    "def shift_target_time_one_side (df, target_datetime, shifted_datetime, side_datetime):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function get a time series, start and end time and\n",
    "    compress or elongate withing a given new date and time and pass the dataframe out\n",
    "    \"\"\"\n",
    "    \n",
    "    if (datetime.strptime(target_datetime, '%Y-%m-%d %H:%M:%S') < \\\n",
    "    datetime.strptime(side_datetime, '%Y-%m-%d %H:%M:%S')) and \\\n",
    "    (datetime.strptime(shifted_datetime, '%Y-%m-%d %H:%M:%S') < \\\n",
    "    datetime.strptime(side_datetime, '%Y-%m-%d %H:%M:%S')):\n",
    "        \n",
    "        # slice the dataframe within the start and end time\n",
    "        df_slice = df [target_datetime:side_datetime]\n",
    "\n",
    "        # upsample to minute frequency\n",
    "        df_resampled = df_slice.resample('T').asfreq()\n",
    "\n",
    "        # interpolate missing values\n",
    "        df_interpolated = df_resampled.interpolate(method='linear')\n",
    "\n",
    "        # create time index of the rescale time frame\n",
    "        time_index = pd.date_range(start=shifted_datetime, end=side_datetime)\n",
    "        \n",
    "    elif (datetime.strptime(target_datetime, '%Y-%m-%d %H:%M:%S') > \\\n",
    "    datetime.strptime(side_datetime, '%Y-%m-%d %H:%M:%S')) and \\\n",
    "    (datetime.strptime(shifted_datetime, '%Y-%m-%d %H:%M:%S') > \\\n",
    "    datetime.strptime(side_datetime, '%Y-%m-%d %H:%M:%S')):\n",
    "        \n",
    "        # slice the dataframe within the start and end time\n",
    "        df_slice = df [side_datetime:target_datetime]\n",
    "\n",
    "        # upsample to minute frequency\n",
    "        df_resampled = df_slice.resample('T').asfreq()\n",
    "\n",
    "        # interpolate missing values\n",
    "        df_interpolated = df_resampled.interpolate(method='linear')\n",
    "\n",
    "        # create time index of the rescale time frame\n",
    "        time_index = pd.date_range(start=side_datetime, end=shifted_datetime)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('cannot decide the case')\n",
    "    \n",
    "    # get the location of resampled of the time series based on the frequency of the new two date and time\n",
    "    space = np.linspace(0, len(df_interpolated)-1, len(time_index))\n",
    "    space = space.astype(int)\n",
    "    \n",
    "    # get the interpolated values and reindex them based on new start and end time for rescale\n",
    "    df_interpolated_reindex = df_interpolated.iloc[space]\n",
    "    df_interpolated_reindex.index = time_index\n",
    "    mask = ~df.index.isin(df_interpolated_reindex.index)\n",
    "    df = df.loc[mask] \n",
    "    df = pd.concat ([df, df_interpolated_reindex])\n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df\n",
    "    \n",
    "def shift_value_target_time (df, target_time, start_time, end_time, scale):\n",
    "    \n",
    "    #\n",
    "    df_temp = df.copy()\n",
    "    df_temp ['w'] = np.nan\n",
    "    df_temp.loc[target_time,'w'] = 1\n",
    "    df_temp.loc[:start_time,'w'] = 0\n",
    "    df_temp.loc[end_time:,  'w'] = 0\n",
    "    \n",
    "    df_temp = df_temp.interpolate()\n",
    "    \n",
    "    \n",
    "    for column in df.columns:\n",
    "        df.loc[:,column]  = (1-df_temp.loc[:,'w']) * df.loc[:,column] + \\\n",
    "                             df_temp.loc[:,'w'] * (df.loc[:,column] - scale)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    \n",
    "def shift_value_target_one_side (df, target_time, start_time, scale, power):\n",
    "    \n",
    "    #\n",
    "    df_temp = df.copy()\n",
    "    df_temp ['w'] = np.nan\n",
    "    df_temp.loc[:start_time, 'w'] = 1\n",
    "    df_temp.loc[target_time:,'w'] = 0\n",
    "    \n",
    "    \n",
    "    df_temp = df_temp.interpolate()\n",
    "    \n",
    "    df_temp ['w'] = df_temp ['w']**power\n",
    "    \n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column]  = (1-df_temp ['w']) * df[column]  + df_temp ['w'] * (df[column]  - scale)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "df = pd.read_csv('../data/HYDAT/05HF003_level.csv') # elevation record of lake diefenbaker\n",
    "\n",
    "# set the date as the index\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.set_index('DATE')\n",
    "df = df.loc['2013-06-01':'2013-07-31']\n",
    "\n",
    "\n",
    "lowest_level_dates = [22,23,24,25,26]\n",
    "\n",
    "resume_level_dates = [22,23,24,25,26,27,28,29,30]\n",
    "\n",
    "start_level_dates = [15, 16, 17,18,19,20,21,22]\n",
    "\n",
    "level_decreases = [0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "\n",
    "lowest_level_date_target = [24]\n",
    "\n",
    "combination = np.array(np.meshgrid(start_level_dates, lowest_level_dates, resume_level_dates, level_decreases, lowest_level_date_target)).T.reshape(-1,5)\n",
    "\n",
    "scenarios = pd.DataFrame(combination, columns = ['start_level_dates', 'lowest_level_dates',\\\n",
    "                                                 'resume_level_dates', 'level_decreases',\\\n",
    "                                                 'target_date'])\n",
    "#\n",
    "scenarios = scenarios[scenarios['start_level_dates'] < scenarios['lowest_level_dates']]\n",
    "scenarios = scenarios[scenarios['lowest_level_dates'] < scenarios['resume_level_dates']]\n",
    "\n",
    "#\n",
    "scenarios = scenarios[scenarios['start_level_dates'] < scenarios['target_date']]\n",
    "scenarios = scenarios[scenarios['target_date'] < scenarios['resume_level_dates']]\n",
    "\n",
    "\n",
    "scenarios = scenarios.reset_index()\n",
    "\n",
    "df_scenarios = df.copy()\n",
    "\n",
    "for index, row in scenarios.iterrows():\n",
    "    \n",
    "    start_level_date = \"{:.0f}\".format(row.start_level_dates)\n",
    "    resume_level_date = \"{:.0f}\".format(row.resume_level_dates)\n",
    "    lowest_level_date = \"{:.0f}\".format(row.lowest_level_dates)\n",
    "    level_decrease = row.level_decreases\n",
    "    \n",
    "    \n",
    "    df1 = shift_target_time (df,\\\n",
    "                            '2013-06-24 00:00:00',\\\n",
    "                            '2013-06-'+str(lowest_level_date)+' 00:00:00',\\\n",
    "                            '2013-06-'+str(start_level_date)+' 00:00:00',\\\n",
    "                            '2013-06-'+str(resume_level_date)+' 00:00:00')\n",
    "\n",
    "\n",
    "    df2 = shift_value_target_time (df1,\\\n",
    "                                  '2013-06-'+str(lowest_level_date)+' 00:00:00',\\\n",
    "                                  '2013-06-'+str(start_level_date)+' 00:00:00',\\\n",
    "                                  '2013-06-'+str(resume_level_date)+' 00:00:00',\\\n",
    "                                  level_decrease)\n",
    "    \n",
    "    #\n",
    "    column_name = 'scenario_'+str(start_level_date)+'_'+str(lowest_level_date)+'_'+str(resume_level_date)+'_'+str(level_decrease)\n",
    "    df_scenarios [column_name] = df2['05HF003']\n",
    "\n",
    "print(len(df_scenarios.columns))\n",
    "df_scenarios\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509287ae",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Creating the target storage from the level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d64a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the elevation storage relationship\n",
    "ele_str = pd.read_csv('../data/Elevation_Storage/Storage_Elevation_Diefenbaker.csv')\n",
    "\n",
    "df_scenarios_storage = df_scenarios.copy()\n",
    "\n",
    "for column in df_scenarios_storage.columns:\n",
    "    df_scenarios_storage [column] =    np.interp(df_scenarios_storage [column], ele_str['Elevation(m)'], ele_str['Storage(m^3)'])\n",
    "print(df_scenarios_storage)\n",
    "\n",
    "df_scenarios_storage = df_scenarios_storage.loc['2013-06-01':'2013-07-31']\n",
    "df_scenarios_storage.to_csv('../data/Storage_ensemble_Scenarios.csv')\n",
    "\n",
    "# plotting\n",
    "df_scenarios_storage['2013-06-15':'2013-07-10'].plot(figsize=(20,10))\n",
    "plt.grid(which='both')\n",
    "plt.gca().get_legend().remove()\n",
    "plt.ylabel('Storage of \\n lake Diefenbaker [m3]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f00066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "myenv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
